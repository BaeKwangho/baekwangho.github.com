---
title: "genBERT 논문 후기"
layout: single
categories:
  - mwp
---

표현식으로 치환해서 모델이 예측하게 하는 것은 여러 문제점이 따른다.
1) 수적 표현이 이산적이고 공간이 같이 커지는 상황에서 모델이 최적화하기 힘든 미분 불가능한 연산자를 사용하여 이런 공간을 찾는 것을 학습하여야 한다.
2) 수적 표현을 사용하지만 최종 output이 span 인 경우, 한계에 봉착한다.

또한, 기존의 LM 모델을 변형하여 numerical reasoning의 역할을 수행하는 head를 사용한 것들도 문제가 있다.
1) 숫자를 9까지 밖에 못 세며, 몇 개 안되는 수에 대해 연산이 가능하다.
2) 학습이 모든 표현에 대해 정답을 이끌어 내기 위한 최대의 확률을 배우기 때문에 지원 가능한 수적 연산의 공간을 늘리는 건 중요하지 않다. (아마 100, 10 그런 거인듯)
3) 문장과 수적 연산의 상호 작용이 있을 수 있기 때문에, 계산기를 모델 밖에서 사용하는 것은 제한적이다.

모델에서는, decoder의 source attention weight가 새로이 학습되는 것을 방지하기 위해 encoder의 self-attention weight를 decoder의 source-attention weight으로써 사용한다고 함. 