---
title: "graph2Tree 논문 후기"
layout: single
categories:
  - mwp
---

> 으흠.. MWP를 푸는 tree 기반의 모델들이 order information이나 relationship을 파악하지 못하는 것에서, 해당 논문에선 graph2tree를 제안하여 이를 가능하게 함. 2가지 데이터셋에 대해 sota를 찍음

# 1. 서론
ml 모델로 여러 시도가 있었으나,, 적합한 features와 expression template이 필요했음.  mwp를 푸는데에 딥러닝 모델들이 차차 나오고, 명시적이거나 암묵적인 tree를 사용하기도 했음. table1에서 보는 것처럼 양의 표현을 개선시키는 게 중요함.
양에 대한 정보가 풍부하기 때문에, 설명 가능한 단어간의 양과 관련된 관계를 모델링하는 것이 중요함. 그래서 두 논문에 기인하여 quantity cell graph만듦.
n1,n2와 같이 치환하여 쓰면,, solution을 생성할 때 문제가 될 수 있음. (table1에서, 384-470 라면?) 그래서 quantity comparison graph를 씀.
또, 같은 수가 수식에서 또 등장하는 경우 RNN 계열 기반의 모델은 이를 놓질 수 있다. 이를 graph encoder로 tree decoder에게 명확이 전달해줄 수 있는 걸 제안함.

# 2. problem formulation
목표는 P의 quantities를 표현 Ep로 맵핑시키는 것.  Ep는 solution 표현 트리 T로 표현될 수 있음. T는 연산자 및 양을 포함하고 있음. Vcon은 여러가지 특수 상수, Vop는 사칙연산을 포함함.

# 3. Methodology
## 3.1 encoder
node representation을 초기화하기 위해 biLSTM을 사용하여 MWP 속의 hidden state representation(hsr)을 학습한다. N=m(word)+l(quantity). 학습된 hsr은 graph encoder의 입력으로 사용된다.
MWP P는 quantity cells QC로 변환된다. m은 P에 속하는 quantity의 개수이다. (즉, 각 quantity 마다 하나의 cell) QC에 포함된 Qi는 quantity token, ni와 그에 맞는 속성 v1i...vqi를 포함한다.  그리고  stanford corenlp를 써서 parsing하여 quantity cell 을 다음과 같은 속성들을 포함하게끔 만든다. (parsing을 써서 quantity 기준에 관련된 각각의 tagged properties를 cell에 포함하는 듯) 만약 아무 속성도 검출하지 못할 경우 quantity 주변의 단어들을 선택한다.
cell graph의 목적은 정보를 지닌 단어들을 연관짓기 위함이고,  comparison graph의 목적은 quantity의 numerical qualities를 확보하여 quantity 사이의 관계를 잘 찾아내기 위함이다.
quantity cell graph는 그냥.. QC의 각 노드들을 (ni와 vni) 다 이어준 그래프라고 보면 되는데, comparison graph는 n간의 (quantity) 수적 대소비교를 방향으로 한 그래프이다. **이는 대소기반의 heuristic한 제약이 음수를 만들어 내는 등의 오류를 방지할 수 있도록 한다.**
***그리고 두 그래프에 대한 인접 행렬을 가져와 N^2 dimensional한 행렬 A로써 초기화한다. (N은 봤다시피 text 길이) i와 j 번째 노드간의 edge가 존재하면, 1을 할당한다. 그밖엔 0이다. 이는 Aqcomp , Aqcell로 사용된다. (즉, 인접 행렬의 형태는 A_i,j에 대해 똑같으나, 내용의 값은 Aqcomp는 Gqcomp를, Aqcell은 Gqcell을 기반으로 할당된다는 의미. 그리고 quantity K개의 각 그래프가 존재할 수 있는 것. 혹은 복사본...)
우선 GCN transformer로 언급된 수식에선, self conv를 n/k에 대해 진행하여 concat해주어 Z를 구한다고 함.  Zbar는 양, 개체, 관계들을 나타냄. 전역 그래프 표현을 학습하기위해 element wise pooling 한걸 한번 더 학습함..
## 3.2 Decoder
Qroot를 zg로 하여 attention을 연산하고 global graph vector Gc를 구함. 아마 나머지 부분들은.. GTS랑 동일하게 흘러가는 듯 함. 다만 모든 연산에서 Gc를 사용하여 노드를 생성하는 듯.
