---
title: "pp-ocr 논문 후기"
layout: single
categories:
  - ocr
---
> ㅋㅋ.. 보아하니 지쳐서 급작스레 그만 둔 느낌이 강하다... 이때 한창 지식이 없었으니 지루할 만도 했지.

# < Abstract >
- OCR은 문자의 외형과 컴퓨터 연산의 효율성에 따라 크게 좌우되었다.
- 해당 논문에서는 경량화된 OCR 시스템을 고안한다.
- 인식 모델, 방향 분류 모델, 탐지 모델 각각 pre-trained 된 것들을 publish 하였다.
- 다른 언어에 대해서도 실행해둔 결과를 포함한다.

# < 1. Introduction >
- 다양한 OCR 사례를 이야기 함.
(difficulties of ocr)
1) Challenges 1. various of text appearances : 문서 내의 글과 실제 세계의 글들로 분류된다.
    - 고밀접과 긴 글에 대한 문제들이 해결해야할 점으로 남아있다. 또한, 문서내 이미지 텍스트 인식은 결과를 구조화 한다는 것에 있어서 새로이 어려운 과제가 되었다.

2) Challenges 2. computational efficiency : 실사용에 있어 처리되야 하는 이미지는 아주 많으며, 이것은 ocr 시스템을 디자인 할때 컴퓨터 연산을 아주 효율적으로 구성하게끔 한다.
    - CPU 연산을 요하는 많은 임베디드 환경에서는 경량화된 모델이 필수적이다. 이를 위해 저자들은 세가지 부분으로 된 pp-ocr을 소개한다; 글자 탐지, 방향 분류, 글자 인식.

(main parts of pp-ocr)
1) 글자 탐지 : 파편화 network인 Differentiable Binarization을 글자 탐지로 사용하고, 6개의 전략을 사용하였다. 결과적으로 탐지기의 size는 1.4M로 줄었다.
2) 탐지된 박스를 바로잡기 : 인식하기 전에, 수평적으로 박스를 변환시켜야 한다. 4개의 포인트로 된 탐지 프레임으로써는 쉬운일이지만, 반대방향으로 수정될 수도 있다. 그러므로, 글자의 방향을 확정해야한다. 만약 박스가 반대로 되어있다고 판단되면, 거꾸로 뒤집는 것이 필요하다.
4가지의 전략을 사용하여 500kb로 줄일 수 있었다.
3) 글자 인식 : 흔히 사용되는 CRNN을 채택하였다. 특징추출과 연속성 모델을 포함하며 예측과 label 사이의 불일치를 피하기 위해 CTC loss를 채택하고 있다. 9가지의 전략을 사용하여 1.6M 정도로 크기를 줄였다.

- 현실적인 OCR 시스템을 실행시키기 위해 대규모의  영/중 인식 데이터셋을 마련하였다; 97K개의 글자 탐지 데이터셋, 600K의 방향 탐지 데이터셋, 17.9M개의 글자 인식 데이터셋.
- 섹션 2에서는 각 모델을 강화시키기 위한 전략들을, 섹션 3에서는 결과를 , 4는 결론을 보여줄 것이다.

# < 2. Enhancement or Sliming Stratgies>
## 2.1) Text Detection
- Light Backbone : mobilenet v1,2,3과 shufflenetv2를 사용하였다. 각각은 다른 크기로 구성되었고 inference time에 기반하여 paddleclas에서 지원하는 백본에 대해서 figure 6와 같은 결과를 얻을 수 있었다. Mobilenetv3_large_x0.5 모델을 정확도와 효율 면에서 채택하기로 하였다.
- Light Head : 객체 탐지에서의 FPN과 비슷한 기능을 하며 특징맵들의 다양한 크기를 녹여내 작은 영역에서의 탐지 능력을 향상시킨다. 이때 서로 다른 특징맵을 합치기 편하게 하기 위해 1x1 conv 를 주로 사용하며 같은 수의 채널을 사용한다. 융합된 서로다른 크기의 특징맵들에서 conv를 사용하기 때문에, inner channel의 감소는 약간의 성능감소로 반틈이상의 모델 크기를 줄일 수 잇었다.

- Remove SE : SE 모델을 backbone, 그러니까 mobilenet에서 제거한다. 원래는 들어있긴한데, 640x640과 같은 큰 이미지에서 SE block으로 channel 단 특징 response를 예측하는 것은 어렵다. 결과적으로 없애보니 정확도 차이 없이 모델 크기만 줄일 수 있었다.
- Cosine Learning Rate Decay : 그냥 천천히 줄여서 성능이 좋았다는 이론이다.
- Learning Rate Warm-up : 음 학습과정이 안정화되면 초기 learning rate를 사용한다는거 같다.
- FPGM Prunner : 모델 가지치기로 성능이 떨어질 것을 피하기 위해 FPGM을 사용한다. 이는 기존 모델에 중요하지 않은 sub-network를 찾아준다.지형적 중앙값을 기준점으로 정하고 각 conv layer 의 필터들은 euclidean 공간에서 고려된다. 뭐 대충 그 비슷한 값을 필터링하는 건듯. 어쨌든 pruning을 하는 전략

## 2.2) Direction Classification
- Light Backbone : 문장 인식과 동일
- Data Augmentation : Yu et al. 2020 에서 BDA라고 불리는 다양한 이미지 처리를 학습데이터에 random하게 더하였다. 그밖에도 이미지 분류를 위한 다양한 augmentation 방법론이 소개됨. 하지만 실험은 Randaugment와 Randerasing을 제외하곤 방향 분류에 있어서는 크게 도움되지 않았다.  결과적으로 BDA와 RandAugment를 방향 분류를 위한 학습 이미지에 추가하였다.
- Input Resolution : 정규화된 이미지들의 입력 해답이 늘수록 (그냥 정답 데이터셋 늘수록) 정확도 올라가는데, backbone이 작으니까  데이터 많아도 계산량이 크게 오르진 않음.  정확도 개선을 위해 조금 더 큰 높이 너비의 이미지를 사용하였음.

- PACT Quantization : 잘 모르겠지만 quantization 자체는 int로 연산하도록 수를 바꾸는 것이라고 하는듯. (뭐래 그냥 우리가 아는 양자화임) 이를 통해 빠른 응답속도, 작은 크기, 적은 연산을 가능하게 함. 기존 방법은 offline / online으로 나뉘어 진행되었음.
근데 계속 보다보니 이거... activation function의 기능을 하는거같음. PACT도 보면 RELU 기반에서 구현된 식이 보임.

## 2.3) Text Recognition : 제일 중요한 문장 인식. 9가지나 되는 전략이 사용되었다. 
- Light Backbone : 이번에도 MobileNet을 이용했다.
- Data Augmentation : BDA와 같이 TIA라는 기술이 글자 인식을 위한 데이터 증강법으로 효과적이다. fig11을 보면 이미지에 초기 포인트를 지정하고, 랜덤하게 움직여서 새로운 이미지를 생성하게 된다. 따라서 글자인식에는 BDA와 TIA를 사용한다.
- Cosine Learning Rate Decay : 글자 탐지에서 말한것과 동일
- Feature Map Resolution : 다국어 인식을 위해 CRNN의 가로 세로가 32, 320으로 설정된다. 이때 기존의 mobilenetv3에 stride를 주는 것은 글자 인식에 적절하지 않다. .... 중략 적절하게 stride를 수정하여 수직/수평 정보를 더 담았음.

- Regularization parameters : 과적합을 피하기 위해 weight_decay를 사용한다. 마지막 loss function 후 L2 regularization이 추가된다. 그로인해 네트워크의 가중치들이 작은 값들을 선택할 수 있게되고 전체적으로 parameter가 0에 가깝게 되며 generalization의 성능이 점진적으로 증가하게 된다. 글자 인식에서 진짜 엄청 큰 효과를 거둔다.
- Learning Rate Warm-up : 똑같다 
- Light Head : 보통 연속 특징을 예측 단어로 encode 하기 위해 fc레이어를 사용하는데, 이때 단어 개수가 많은 중국어와 같은 경우 연속 특징의 dimension 수가 굉장히 큰 영향을 끼친다. 하지만 더 크다고 해서 연속 특징을 잘 표현하는 것도 아니라서 48개로 고정한다고 함.
- Pre-trained Model : 학습 데이터가 작으면 ImageNet을 사전 학습한 걸 fine tune 하여 수렴 빠르고 정확도 높일 수 있다. 전이 학습에서 이것들은 굉장히 효과적임. 특히 real scene에서 누가 제한 걸은 데이터도 이 방식으로 하면 좋은 결과를 얻을 수 있음.